{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e43502",
   "metadata": {},
   "source": [
    "# Profit Forecasting with neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3cfee5",
   "metadata": {},
   "source": [
    "This notebook contains the implementation of a MLP. The pre processing of the data is done in the same way as in the main pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776bc23",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34af643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# pre processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# neural network packages \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# metrics \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4675c0",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# feature selection \n",
    "data = data.drop(['id', 'country'], axis=1)\n",
    "\n",
    "# one hot encoding\n",
    "data = pd.get_dummies(data=data, columns=['ship_mode', 'segment', 'region', 'state', 'city', 'postal_code', 'category', 'sub_category'])\n",
    "\n",
    "X = data.drop('profit', axis=1)\n",
    "y = data['profit']\n",
    "\n",
    "# train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling of numerical features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[['discount', 'sales', 'quantity']])\n",
    "X_train[['discount', 'sales', 'quantity']] = scaler.transform(X_train[['discount', 'sales', 'quantity']])\n",
    "X_test[['discount', 'sales', 'quantity']] = scaler.transform(X_test[['discount', 'sales', 'quantity']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f62b6",
   "metadata": {},
   "source": [
    "## Implementation of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96bb753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_170 (Dense)           (None, 921)               849162    \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 30)                27660     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 30)                0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 30)                0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 30)                930       \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 30)                0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 878,713\n",
      "Trainable params: 878,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 55.4046\n",
      "Epoch 1: val_loss improved from inf to 48.22356, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 56.8263 - val_loss: 48.2236\n",
      "Epoch 2/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 53.6888\n",
      "Epoch 2: val_loss improved from 48.22356 to 44.96060, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 53.6888 - val_loss: 44.9606\n",
      "Epoch 3/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 50.0318\n",
      "Epoch 3: val_loss improved from 44.96060 to 42.88183, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 50.0134 - val_loss: 42.8818\n",
      "Epoch 4/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 46.7712\n",
      "Epoch 4: val_loss improved from 42.88183 to 40.19840, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 46.7712 - val_loss: 40.1984\n",
      "Epoch 5/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 43.8734\n",
      "Epoch 5: val_loss improved from 40.19840 to 39.13523, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 43.8734 - val_loss: 39.1352\n",
      "Epoch 6/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 40.4475\n",
      "Epoch 6: val_loss improved from 39.13523 to 38.89876, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 40.4476 - val_loss: 38.8988\n",
      "Epoch 7/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 39.6300\n",
      "Epoch 7: val_loss improved from 38.89876 to 38.70807, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 39.6300 - val_loss: 38.7081\n",
      "Epoch 8/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 36.9282\n",
      "Epoch 8: val_loss improved from 38.70807 to 37.75293, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 36.9601 - val_loss: 37.7529\n",
      "Epoch 9/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 37.1084\n",
      "Epoch 9: val_loss improved from 37.75293 to 37.52875, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 36.9476 - val_loss: 37.5288\n",
      "Epoch 10/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 34.6378\n",
      "Epoch 10: val_loss improved from 37.52875 to 37.52539, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 34.6177 - val_loss: 37.5254\n",
      "Epoch 11/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 34.3298\n",
      "Epoch 11: val_loss improved from 37.52539 to 37.05684, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 34.2628 - val_loss: 37.0568\n",
      "Epoch 12/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 33.2436\n",
      "Epoch 12: val_loss did not improve from 37.05684\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 33.2565 - val_loss: 37.9844\n",
      "Epoch 13/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 32.8850\n",
      "Epoch 13: val_loss did not improve from 37.05684\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 32.7383 - val_loss: 37.3551\n",
      "Epoch 14/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 32.5246\n",
      "Epoch 14: val_loss did not improve from 37.05684\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 32.2536 - val_loss: 37.2678\n",
      "Epoch 15/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 30.7703\n",
      "Epoch 15: val_loss improved from 37.05684 to 36.86682, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 30.6093 - val_loss: 36.8668\n",
      "Epoch 16/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 30.6375\n",
      "Epoch 16: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 30.6294 - val_loss: 37.5163\n",
      "Epoch 17/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 30.3590\n",
      "Epoch 17: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 30.4783 - val_loss: 36.9545\n",
      "Epoch 18/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 29.9920\n",
      "Epoch 18: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 29.9919 - val_loss: 37.6787\n",
      "Epoch 19/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 29.2991\n",
      "Epoch 19: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 29.0213 - val_loss: 40.4993\n",
      "Epoch 20/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 28.6854\n",
      "Epoch 20: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 29.4139 - val_loss: 37.6928\n",
      "Epoch 21/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 28.4863\n",
      "Epoch 21: val_loss did not improve from 36.86682\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 28.5602 - val_loss: 38.1634\n",
      "Epoch 22/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 28.7977\n",
      "Epoch 22: val_loss improved from 36.86682 to 36.85007, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 28.5190 - val_loss: 36.8501\n",
      "Epoch 23/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 27.7070\n",
      "Epoch 23: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 27.9586 - val_loss: 38.7210\n",
      "Epoch 24/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 26.2796\n",
      "Epoch 24: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 26.4113 - val_loss: 38.5008\n",
      "Epoch 25/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 27.2899\n",
      "Epoch 25: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 27.1940 - val_loss: 38.9702\n",
      "Epoch 26/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 26.9680\n",
      "Epoch 26: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 27.1561 - val_loss: 36.9341\n",
      "Epoch 27/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 27.3561\n",
      "Epoch 27: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 27.1926 - val_loss: 37.6329\n",
      "Epoch 28/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 26.1949\n",
      "Epoch 28: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 26.1380 - val_loss: 38.2930\n",
      "Epoch 29/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 26.3133\n",
      "Epoch 29: val_loss did not improve from 36.85007\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 26.3133 - val_loss: 37.9453\n",
      "Epoch 30/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 25.1376\n",
      "Epoch 30: val_loss improved from 36.85007 to 36.84110, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 25.1376 - val_loss: 36.8411\n",
      "Epoch 31/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 25.9945\n",
      "Epoch 31: val_loss did not improve from 36.84110\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 25.9945 - val_loss: 37.9755\n",
      "Epoch 32/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 24.6486\n",
      "Epoch 32: val_loss did not improve from 36.84110\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.4802 - val_loss: 38.5624\n",
      "Epoch 33/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 25.1586\n",
      "Epoch 33: val_loss did not improve from 36.84110\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 25.0595 - val_loss: 38.1030\n",
      "Epoch 34/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 26.6498\n",
      "Epoch 34: val_loss did not improve from 36.84110\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 26.2960 - val_loss: 37.3856\n",
      "Epoch 35/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 23.7567\n",
      "Epoch 35: val_loss improved from 36.84110 to 36.75089, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.1544 - val_loss: 36.7509\n",
      "Epoch 36/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 24.7731\n",
      "Epoch 36: val_loss did not improve from 36.75089\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.5628 - val_loss: 37.4039\n",
      "Epoch 37/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 24.5061\n",
      "Epoch 37: val_loss did not improve from 36.75089\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.6636 - val_loss: 37.3382\n",
      "Epoch 38/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 24.7116\n",
      "Epoch 38: val_loss did not improve from 36.75089\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.5256 - val_loss: 38.5398\n",
      "Epoch 39/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 24.3267\n",
      "Epoch 39: val_loss improved from 36.75089 to 36.54087, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.3267 - val_loss: 36.5409\n",
      "Epoch 40/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 24.7456\n",
      "Epoch 40: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.6028 - val_loss: 36.8530\n",
      "Epoch 41/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 24.9802\n",
      "Epoch 41: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 25.4265 - val_loss: 38.0358\n",
      "Epoch 42/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 23.6794\n",
      "Epoch 42: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.8492 - val_loss: 38.2851\n",
      "Epoch 43/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 23.5925\n",
      "Epoch 43: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.6236 - val_loss: 38.7296\n",
      "Epoch 44/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 24.7860\n",
      "Epoch 44: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.6240 - val_loss: 37.0631\n",
      "Epoch 45/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 23.1562\n",
      "Epoch 45: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.0262 - val_loss: 37.1370\n",
      "Epoch 46/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 23.2608\n",
      "Epoch 46: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.5569 - val_loss: 37.9000\n",
      "Epoch 47/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 24.3610\n",
      "Epoch 47: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.2801 - val_loss: 37.3397\n",
      "Epoch 48/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 23.1910\n",
      "Epoch 48: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.9735 - val_loss: 37.7356\n",
      "Epoch 49/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 23.2118\n",
      "Epoch 49: val_loss did not improve from 36.54087\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.1968 - val_loss: 38.1140\n",
      "Epoch 50/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 24.7123\n",
      "Epoch 50: val_loss improved from 36.54087 to 36.45826, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 24.4224 - val_loss: 36.4583\n",
      "Epoch 51/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 23.2113\n",
      "Epoch 51: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.1929 - val_loss: 37.2357\n",
      "Epoch 52/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 22.6445\n",
      "Epoch 52: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.9799 - val_loss: 36.4664\n",
      "Epoch 53/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 22.6282\n",
      "Epoch 53: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.6282 - val_loss: 36.6785\n",
      "Epoch 54/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 23.2797\n",
      "Epoch 54: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.2633 - val_loss: 36.9789\n",
      "Epoch 55/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 23.0573\n",
      "Epoch 55: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 23.0999 - val_loss: 36.6798\n",
      "Epoch 56/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 22.4697\n",
      "Epoch 56: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.4901 - val_loss: 38.0498\n",
      "Epoch 57/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 22.2722\n",
      "Epoch 57: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.2722 - val_loss: 36.7134\n",
      "Epoch 58/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.3263\n",
      "Epoch 58: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.8345 - val_loss: 37.5645\n",
      "Epoch 59/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 23.0902\n",
      "Epoch 59: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.8153 - val_loss: 38.5992\n",
      "Epoch 60/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 21.9840\n",
      "Epoch 60: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.9924 - val_loss: 36.8075\n",
      "Epoch 61/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 21.7733\n",
      "Epoch 61: val_loss did not improve from 36.45826\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.5993 - val_loss: 36.8068\n",
      "Epoch 62/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 21.3237\n",
      "Epoch 62: val_loss improved from 36.45826 to 36.43623, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.5928 - val_loss: 36.4362\n",
      "Epoch 63/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 22.5319\n",
      "Epoch 63: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.5191 - val_loss: 37.2047\n",
      "Epoch 64/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 21.6686\n",
      "Epoch 64: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.6695 - val_loss: 37.9361\n",
      "Epoch 65/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 21.3901\n",
      "Epoch 65: val_loss did not improve from 36.43623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 1s 8ms/step - loss: 21.5810 - val_loss: 36.9032\n",
      "Epoch 66/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 22.1331\n",
      "Epoch 66: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.9139 - val_loss: 36.7088\n",
      "Epoch 67/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 22.7547\n",
      "Epoch 67: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.7547 - val_loss: 36.6031\n",
      "Epoch 68/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 21.6682\n",
      "Epoch 68: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.6682 - val_loss: 36.6624\n",
      "Epoch 69/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.7016\n",
      "Epoch 69: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.7054 - val_loss: 36.8647\n",
      "Epoch 70/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 21.1517\n",
      "Epoch 70: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.2771 - val_loss: 37.3433\n",
      "Epoch 71/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 21.4481\n",
      "Epoch 71: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.4481 - val_loss: 36.9137\n",
      "Epoch 72/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 21.6112\n",
      "Epoch 72: val_loss did not improve from 36.43623\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.5977 - val_loss: 36.9643\n",
      "Epoch 73/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 22.0621\n",
      "Epoch 73: val_loss improved from 36.43623 to 36.14560, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.2470 - val_loss: 36.1456\n",
      "Epoch 74/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 20.7119\n",
      "Epoch 74: val_loss improved from 36.14560 to 36.08848, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.5648 - val_loss: 36.0885\n",
      "Epoch 75/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.4780\n",
      "Epoch 75: val_loss did not improve from 36.08848\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.4188 - val_loss: 36.4173\n",
      "Epoch 76/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 21.3171\n",
      "Epoch 76: val_loss improved from 36.08848 to 35.95520, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.3225 - val_loss: 35.9552\n",
      "Epoch 77/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.6631\n",
      "Epoch 77: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.5667 - val_loss: 36.4042\n",
      "Epoch 78/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.1098\n",
      "Epoch 78: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.9452 - val_loss: 37.8140\n",
      "Epoch 79/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 22.2251\n",
      "Epoch 79: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 22.2251 - val_loss: 36.3595\n",
      "Epoch 80/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 20.9527\n",
      "Epoch 80: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.8610 - val_loss: 37.5155\n",
      "Epoch 81/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 21.7297\n",
      "Epoch 81: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 21.8605 - val_loss: 37.6964\n",
      "Epoch 82/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 20.0707\n",
      "Epoch 82: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.3451 - val_loss: 37.2620\n",
      "Epoch 83/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 20.8316\n",
      "Epoch 83: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.9699 - val_loss: 36.6235\n",
      "Epoch 84/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 21.0162\n",
      "Epoch 84: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.8118 - val_loss: 36.5416\n",
      "Epoch 85/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 20.0976\n",
      "Epoch 85: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.5479 - val_loss: 36.3100\n",
      "Epoch 86/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 20.0885\n",
      "Epoch 86: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.0910 - val_loss: 36.7619\n",
      "Epoch 87/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 20.0791\n",
      "Epoch 87: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.2183 - val_loss: 36.1213\n",
      "Epoch 88/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 20.3812\n",
      "Epoch 88: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.4016 - val_loss: 35.9564\n",
      "Epoch 89/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 20.0081\n",
      "Epoch 89: val_loss did not improve from 35.95520\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.3110 - val_loss: 36.4466\n",
      "Epoch 90/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 20.9773\n",
      "Epoch 90: val_loss improved from 35.95520 to 35.91817, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.9014 - val_loss: 35.9182\n",
      "Epoch 91/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 19.8092\n",
      "Epoch 91: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.7985 - val_loss: 35.9878\n",
      "Epoch 92/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 20.3301\n",
      "Epoch 92: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.2372 - val_loss: 36.3871\n",
      "Epoch 93/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 19.9002\n",
      "Epoch 93: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.9009 - val_loss: 37.1650\n",
      "Epoch 94/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 20.8230\n",
      "Epoch 94: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.8071 - val_loss: 36.6660\n",
      "Epoch 95/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 20.0640\n",
      "Epoch 95: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.1010 - val_loss: 36.1652\n",
      "Epoch 96/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 19.7320\n",
      "Epoch 96: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.6539 - val_loss: 36.6366\n",
      "Epoch 97/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.9512\n",
      "Epoch 97: val_loss did not improve from 35.91817\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.9512 - val_loss: 36.1127\n",
      "Epoch 98/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 19.9135\n",
      "Epoch 98: val_loss improved from 35.91817 to 35.91755, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.9116 - val_loss: 35.9176\n",
      "Epoch 99/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 19.9777\n",
      "Epoch 99: val_loss did not improve from 35.91755\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.8349 - val_loss: 36.0148\n",
      "Epoch 100/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 18.8282\n",
      "Epoch 100: val_loss did not improve from 35.91755\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.7208 - val_loss: 36.3919\n",
      "Epoch 101/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 19.5226\n",
      "Epoch 101: val_loss did not improve from 35.91755\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.6154 - val_loss: 36.7505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 19.5162\n",
      "Epoch 102: val_loss improved from 35.91755 to 34.62108, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.3216 - val_loss: 34.6211\n",
      "Epoch 103/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.6591\n",
      "Epoch 103: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.6591 - val_loss: 35.9334\n",
      "Epoch 104/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 18.6943\n",
      "Epoch 104: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.6853 - val_loss: 35.0900\n",
      "Epoch 105/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.8804\n",
      "Epoch 105: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.8804 - val_loss: 36.3285\n",
      "Epoch 106/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 19.6881\n",
      "Epoch 106: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.7083 - val_loss: 36.4996\n",
      "Epoch 107/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 19.1336\n",
      "Epoch 107: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.1894 - val_loss: 36.3228\n",
      "Epoch 108/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.1518\n",
      "Epoch 108: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.1518 - val_loss: 35.7219\n",
      "Epoch 109/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 18.4235\n",
      "Epoch 109: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.9708 - val_loss: 36.1328\n",
      "Epoch 110/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 19.3594\n",
      "Epoch 110: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.3594 - val_loss: 37.1580\n",
      "Epoch 111/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 19.0989\n",
      "Epoch 111: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.1744 - val_loss: 34.7093\n",
      "Epoch 112/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 18.2577\n",
      "Epoch 112: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.1712 - val_loss: 35.4886\n",
      "Epoch 113/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 19.4006\n",
      "Epoch 113: val_loss did not improve from 34.62108\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.3364 - val_loss: 35.0049\n",
      "Epoch 114/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 19.6841\n",
      "Epoch 114: val_loss improved from 34.62108 to 34.30316, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.4617 - val_loss: 34.3032\n",
      "Epoch 115/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 19.1964\n",
      "Epoch 115: val_loss improved from 34.30316 to 33.77101, saving model to my_best_model.hdf5\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.0712 - val_loss: 33.7710\n",
      "Epoch 116/150\n",
      "172/173 [============================>.] - ETA: 0s - loss: 19.5970\n",
      "Epoch 116: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.5976 - val_loss: 36.6070\n",
      "Epoch 117/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 20.2979\n",
      "Epoch 117: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.1194 - val_loss: 36.8472\n",
      "Epoch 118/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 18.4258\n",
      "Epoch 118: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.3701 - val_loss: 34.9386\n",
      "Epoch 119/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 20.3436\n",
      "Epoch 119: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 20.3450 - val_loss: 36.5970\n",
      "Epoch 120/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 19.6166\n",
      "Epoch 120: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.5964 - val_loss: 36.3269\n",
      "Epoch 121/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 18.4525\n",
      "Epoch 121: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.8088 - val_loss: 35.9396\n",
      "Epoch 122/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.9196\n",
      "Epoch 122: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.8442 - val_loss: 34.6710\n",
      "Epoch 123/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 18.4983\n",
      "Epoch 123: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.3622 - val_loss: 35.4636\n",
      "Epoch 124/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 19.4672\n",
      "Epoch 124: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.6589 - val_loss: 37.2630\n",
      "Epoch 125/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 19.4350\n",
      "Epoch 125: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.3872 - val_loss: 36.9395\n",
      "Epoch 126/150\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 18.5751\n",
      "Epoch 126: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.8821 - val_loss: 35.7506\n",
      "Epoch 127/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.7073\n",
      "Epoch 127: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.5540 - val_loss: 34.8262\n",
      "Epoch 128/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 19.2370\n",
      "Epoch 128: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.2027 - val_loss: 35.2494\n",
      "Epoch 129/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.9183\n",
      "Epoch 129: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.7306 - val_loss: 35.6907\n",
      "Epoch 130/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 18.4403\n",
      "Epoch 130: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.3804 - val_loss: 35.5650\n",
      "Epoch 131/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 17.8029\n",
      "Epoch 131: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.7313 - val_loss: 35.1130\n",
      "Epoch 132/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 18.7023\n",
      "Epoch 132: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.9093 - val_loss: 35.4765\n",
      "Epoch 133/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.3544\n",
      "Epoch 133: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.3544 - val_loss: 35.5822\n",
      "Epoch 134/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 17.7475\n",
      "Epoch 134: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.7198 - val_loss: 35.3853\n",
      "Epoch 135/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 18.8257\n",
      "Epoch 135: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.8068 - val_loss: 36.3474\n",
      "Epoch 136/150\n",
      "170/173 [============================>.] - ETA: 0s - loss: 17.8668\n",
      "Epoch 136: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.9297 - val_loss: 36.3324\n",
      "Epoch 137/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 18.3752\n",
      "Epoch 137: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.3771 - val_loss: 36.5796\n",
      "Epoch 138/150\n",
      "168/173 [============================>.] - ETA: 0s - loss: 19.0365\n",
      "Epoch 138: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 19.0782 - val_loss: 34.9396\n",
      "Epoch 139/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 16.9707\n",
      "Epoch 139: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 16.9707 - val_loss: 35.6673\n",
      "Epoch 140/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 17.6054\n",
      "Epoch 140: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 17.6278 - val_loss: 35.5778\n",
      "Epoch 141/150\n",
      "171/173 [============================>.] - ETA: 0s - loss: 17.9909\n",
      "Epoch 141: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 18.0626 - val_loss: 35.4381\n",
      "Epoch 142/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.6727\n",
      "Epoch 142: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 18.5695 - val_loss: 35.3825\n",
      "Epoch 143/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 17.6493\n",
      "Epoch 143: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 17.7158 - val_loss: 35.1257\n",
      "Epoch 144/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.4107\n",
      "Epoch 144: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.4345 - val_loss: 35.1559\n",
      "Epoch 145/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 18.4648\n",
      "Epoch 145: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 18.4271 - val_loss: 35.4904\n",
      "Epoch 146/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 18.2396\n",
      "Epoch 146: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 18.2396 - val_loss: 35.9070\n",
      "Epoch 147/150\n",
      "173/173 [==============================] - ETA: 0s - loss: 17.5327\n",
      "Epoch 147: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 17.5327 - val_loss: 36.5934\n",
      "Epoch 148/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 17.9614\n",
      "Epoch 148: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.8259 - val_loss: 35.4982\n",
      "Epoch 149/150\n",
      "169/173 [============================>.] - ETA: 0s - loss: 17.8844\n",
      "Epoch 149: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.8284 - val_loss: 35.4162\n",
      "Epoch 150/150\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 17.2172\n",
      "Epoch 150: val_loss did not improve from 33.77101\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 17.4141 - val_loss: 35.4231\n",
      "R2_Score: 0.8071344771623581\n",
      "RMSE: 89.70047722965055\n",
      "MAE: 37.19128171974354\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fix random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# initiate neural network\n",
    "model = Sequential()\n",
    "\n",
    "# input layer with 921 neurons\n",
    "model.add(Dense(921, input_dim=921, activation='relu'))\n",
    "\n",
    "# 3 hidden layer with 30 neurons each and dropout function to avoid overfitting\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# output layer with 1 neuron to predict single value \n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# create checkpoint for saving best model \n",
    "checkpoint = ModelCheckpoint(filepath='my_best_model.hdf5', \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks = checkpoint\n",
    "\n",
    "# fit model \n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=150, batch_size=25, verbose=1, callbacks=callbacks)  \n",
    "\n",
    "# predictions\n",
    "model = load_model('my_best_model.hdf5')\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# model evaluation\n",
    "print ('R2_Score:', r2_score(y_test, predictions))\n",
    "print ('RMSE:', mean_squared_error(y_test, predictions, squared=False))\n",
    "print ('MAE:', mean_absolute_error(y_test, predictions))\n",
    "\n",
    "# plot learning curves\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
